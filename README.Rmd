---
title: "README"
output:
  github_document:
    html_preview: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Global COVID-19 Response
_Last updated: 15 Sep 2020_

## Table of Contents
- [About](#About)
- [Goals](#Goals)
- [Modelling technique](#Modelling-technique)
  - [Facility-level models](#Facility-level-models)
  - [District and county-level models](#District-and-county-level-models)
  - [Missing data considerations](#Missing-data-considerations)
- [Overview of folders and files](#Overview-of-folders-and-files)

## About:
This repository contains code to follow the Data Processing Pipeline for the Global Covid-19 Syndromic Surveillance Team - a partnership between sites at Partners in Health, the Global Health Research Core at Harvard Medical School, and Brigham and Women's Hospital. The data has been modified to respect the privacy of our sites, in hopes that other groups can benefit from the functions we have written.

This repository contains data, code, and other items needed to reproduce this work. Outputs include figures, tables, and Leaflet maps. Further explanation of outputs and their construction is given in the "Overview of folders and files" section, which includes detailed explanations of the functions we have written.


## Goals:
The main goal of the Global COVID-19 Syndromic Survillance Team is to monitor changes in indicators that may signal changes in COVID-19 case numbers in health systems from our eight partnering countries: Haiti, Lesotho, Liberia, Madagascar, Malawi, Mexico, Peru, and Rwanda. This is accomplished through establishing a baseline using prior data, and monitoring for deviations for relevant indicators. The data visualization tools created using our functions allow identification of local areas that are experiencing upticks in COVID-19-related symptoms.

## Modelling technique:
The process starting with the raw data and finishing with the various outputs is referred to as the Data Processing Pipeline (see Figure 1 below):

After data has been cleaned, it is processed according to the level it is available at (either on a facility of county/district basis) for each indicator. This is done by taking data from a historic baseline period, and then projecting it into the evaluation period. This then is compared to the observed counts/proportions. A 95% confidence interval has been chosen, and we have defined the baseline period to be data from January 2016-December 2019. 

The functions included in this repository focus on the modelling and processing stages.

### Facility-level models:

For facility-level assessments, we fit a generalized linear model with negative binomial distribution and log-link to estimate expected monthly counts. Only data from the baseline period will be used to estimate the expected counts:
$$ \log(E[Y | year, t ]) = \beta_0 + \beta_1year + \sum_{k=1}^{3} \beta_{k1} cos(2 \pi kt/12) + \beta_{k2} sin(2 \pi kt/12) $$
where Y indicates monthly indicator count, t indicates the cumulative month number. The year term captures trend, and the harmonic term captures seasonality. This model is an adaptation of that proposed by Dan Weinberger lab (CITE). If data is available on a more granular level, then weekly or daily terms could be added to the equation to capture other types of trend. To calculate the prediction intervals, we used ciTools R package (https://cran.r-project.org/web/packages/ciTools/ciTools.pdf).
 
For proportions, in which the numerator is indicator counts and the denominator is outpatient visits, we produced similar prediction intervals using the following procedure: we performed a parametric bootstrap procedure that generates random monthly indicator counts from the prediction intervals described above and kept the total outpatient visits fixed. This gives empirical estimates and prediction intervals for proportions.  If there were missing values for the monthly outpatient visit count, instead of deleting those months and doing a complete-case analysis which would waste existing indicator count data, we performed an imputation procedure as follows: first, we fit the aforementioned model for outpatient visits instead of indicator counts, and using that modelâ€™s estimates, imputed the missing denominator values. Then, we can do the parametric bootstrap procedure with the additional step of randomly imputing missing denominator values in order to account for variation and uncertainty in these imputed outpatient values. 


### District and county-level models:

Alternatively, one could fit a generalized linear mixed model using the above equation with a random effect terms for each facility within the region. The region-level count estimates can then be obtained by integrating over the random effects distribution. In the supplemental material, we provide comparisons of these two proposed methods:
$$ \log(E[Y_j | year, t ]) = \beta_0 ^* + \beta_1^*year + \sum_{k=1}^{3} \beta_{k1}^* cos(2 \pi kt/12) + \beta_{k2}^* sin(2 \pi kt/12) + \gamma _{0j} $$
Importantly, we did not report facility-level estimates using this model. Instead, we estimated the marginal (population-level) total count by integrating over the random effect distribution. The facility-level estimates from this model will NOT match results from the individual facility-level models above. The facility-level models allowed for facility-specific year and seasonality trends, whereas the GLMM assumed common year and seasonality trends across all facilities in the district or county, allowing only the baseline counts to vary by facility via the random intercepts.

### Missing data considerations:
We excluded facilities from our analysis for two reasons: (1) missing dates in the baseline period (creation of the expected counts model) (2) missing observed counts in the evaluation period.

For the first reason, facilities with high levels of missing data (more than 20% of baseline dates missing) were excluded. Although there are statistical methods that can handle missing time series data, we decided to only include sites that demonstrated ability to collect data over time. A complete case (time) analysis was conducted with included facilities, which assumes that the counts were missing completely at random (MCAR). Specifically, we assumed the reason for missing counts was independent of time and count value. If the MCAR assumption was violated and we had information about the missing data mechanism, one could impute values for the missing data and report unbiased expected counts and correct inference.

For the second reason, facilities with ANY missing monthly data during the evaluation period (January 2020 onward) were removed. As the syndromic surveillance exercise hinges on comparing the observed counts to the expected and flagging for deviations, we require complete observed data during this period. In this context, it would be invalid to impute observed counts based on information from the baseline period. In theory, one could attempt to impute the observed count based on information during the evaluation period.

## Overview of folders and files:
### data
This folder contains example data used to demonstrate functions.
#### data.example_singlecounty.rds
The facility-level dataset used to demonstrate the functions throughout this repository. Note- specific names and numbers have been altered to respect the privacy of our sites.

### R
This folder contains the functions used to create the key data visualization figures and maps.

### maps

### figures
This folder contains figures that have been included in README.md.


### Example

#### Loading Data and Functions

```{r}
source("R/model_functions.R")
source("R/model_figures.R")
data <- readRDS("data/data_example_singlecounty.rds")
test <- readRDS("../global_covid19_ss/liberia/data/cleaned/liberia_cleaned_08-25-2020.rds")


```

#### Single Facility

```{r}
#head for this to show what it looks like
# Counts


# Proportion
```


#### All Facilities

```{r}
# lapply, just code
```



#### County-level

```{r}
# no need for proportion here
# 4 examples
```




```{r}
# for figures, for first part, use output to make 2 figures, tile plot and standard thing
# nicole using real data maps
# save output for single facility and multiple facilities to create figures later..another folder called results or output like results_example1 or something
# example 1 facility counts, example 2 facility proportions, example 3 multiple fac, facet wrap for all of em, example 4, county counts, tile thing
# separate r script for figures (heat map, time series single, time series facet wrap)
#
#
```



